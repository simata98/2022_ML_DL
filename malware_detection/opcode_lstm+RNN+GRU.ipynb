{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "corpus = list()\n",
    "labels = list()\n",
    "\n",
    "with open(Path('./output/benign/all_benign.txt'), 'r') as f:\n",
    "  lines = f.read().split('\\n')[:-1]\n",
    "  # print(lines[0][33:])\n",
    "for line in lines:\n",
    "  doc = line[33:]\n",
    "  corpus.append(doc)\n",
    "  labels.append(0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path('./output/malware/all_malware.txt'), 'r') as f:\n",
    "  lines = f.read().split('\\n')[:-1]\n",
    "  # print(lines[0][33:])\n",
    "for line in lines:\n",
    "  doc = line[33:]\n",
    "  corpus.append(doc)\n",
    "  labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, encode_start_end = False):\n",
    "  if encode_start_end:\n",
    "    setneces = [\"문장시작 \" + s + \"문장 끝\" for s in sentences]\n",
    "    \n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(sentences)\n",
    "  tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "  \n",
    "  return tokenized_sentences, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(sentences, length=None):\n",
    "  if length is None:\n",
    "    length = max([len(s) for s in sentences])\n",
    "  \n",
    "  padded_sentences = pad_sequences(sentences,\n",
    "                                   maxlen = length,\n",
    "                                   padding = 'post',\n",
    "                                   truncating = 'post')\n",
    "  \n",
    "  return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized, X_tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = pad(X_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vocab_size = len(X_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 opcode vocab size:  25\n"
     ]
    }
   ],
   "source": [
    "print(\"총 opcode vocab size: \", X_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 opcode 문장 길이:  2800\n"
     ]
    }
   ],
   "source": [
    "X_seq_len = len(X_encoded)\n",
    "print(\"가장 긴 opcode 문장 길이: \", X_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer Encoder_LSTM will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer Decoder_LSTM will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "encoder_input = Input(shape = (None, ),\n",
    "                      name=\"encoder_input\")\n",
    "embedding_dim = 200\n",
    "embedded_input = Embedding(input_dim = X_vocab_size,\n",
    "                           output_dim = embedding_dim,\n",
    "                           name = \"Embedding_Layer\")(encoder_input)\n",
    "encoder_lstm = LSTM(units=256,\n",
    "                    activation  = 'relu',\n",
    "                    return_sequences=False,\n",
    "                    return_state=True,\n",
    "                    name=\"Encoder_LSTM\")\n",
    "\n",
    "_, last_h_encoder, last_c_encoder = encoder_lstm(embedded_input)\n",
    "\n",
    "decoder_input = Input(shape=(None, 1),\n",
    "                      name = 'Decoder_Input')\n",
    "\n",
    "decoder_lstm = LSTM(units=256,\n",
    "                    activation='relu',\n",
    "                    return_sequences=True,\n",
    "                    return_state=True,\n",
    "                    name='Decoder_LSTM')\n",
    "\n",
    "all_h_decoder, _, _ = decoder_lstm(decoder_input,\n",
    "                                   initial_state=[last_h_encoder, last_c_encoder])\n",
    "\n",
    "final_dense = Dense(1, activation='sigmoid', name='final_dense_layer')\n",
    "\n",
    "logits = final_dense(all_h_decoder)\n",
    "\n",
    "seq2seq_model = Model([encoder_input, decoder_input],\n",
    "                      logits)\n",
    "\n",
    "seq2seq_model.compile(loss='binary_crossentopy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "import tqdm\n",
    "\n",
    "decoder_X_input = X_encoded.reshape((-1, X_seq_len, 1))\n",
    "decoder_X_target = labels.reshape((-1, X_seq_len, 1))\n",
    "\n",
    "tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
    "mc = ModelCheckpoint('./log/s2q_model.h5', save_best_only=True)\n",
    "\n",
    "seq2seq_model.fit([X_encoded, decoder_X_input],\n",
    "                  [X],\n",
    "                  epochs = 100,\n",
    "                  batch_size = 512,\n",
    "                  validation_split=0.1,\n",
    "                  callbacks=[mc, tqdm_callback])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "263279b5c7f5294ef7179eeb038e9113bb5dce42ff0c0d19b3e81ce1438dd79a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('keras_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
